# PPO configuration for Trading Bot Simulator

# Inherit from default config
defaults: [default]

train:
  algo: ppo
  total_timesteps: 200000
  learning_rate: 0.0003
  batch_size: 64
  n_steps: 2048
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  clip_range_vf: null
  normalize_advantage: true
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: false
  sde_sample_freq: -1
  target_kl: null
  tensorboard_log: runs/ppo/tensorboard
  # Removed policy_kwargs to avoid PyTorch compatibility issues
